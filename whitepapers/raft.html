<h1 id="raft-in-search-of-an-understandable-consensus-algorithm">RAFT:
In search of an Understandable Consensus Algorithm</h1>
<h2 id="table-of-content">Table of Content</h2>
<ul>
<li><a
href="#raft-in-search-of-an-understandable-consensus-algorithm">RAFT: In
search of an Understandable Consensus Algorithm</a>
<ul>
<li><a href="#table-of-content">Table of Content</a></li>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#replicated-state-machines">Replicated State
Machines</a></li>
<li><a href="#must-have-properties-of-a-consensus-algorithm">Must-have
properties of a consensus algorithm</a></li>
<li><a href="#main-goal-of-raft-">Main GOAL of RAFT ?</a></li>
<li><a href="#the-raft-consensus-algorithm">The Raft Consensus
Algorithm</a>
<ul>
<li><a href="#decomposition-of-consensus-problem">Decomposition of
Consensus Problem</a></li>
</ul></li>
<li><a href="#raft-guarantees---true-at-all-times">Raft Guarantees -
True at all times!</a></li>
<li><a href="#raft-basics">Raft Basics</a></li>
<li><a href="#leader-election">Leader Election</a></li>
<li><a href="#state-of-a-server">State of a Server</a></li>
<li><a href="#appendentries-rpc">AppendEntries RPC</a></li>
<li><a href="#requestvote-rpc">RequestVote RPC</a></li>
<li><a href="#rules-for-servers">Rules for Servers</a></li>
<li><a href="#logs-replication">Logs Replication</a></li>
<li><a href="#safety">Safety</a>
<ul>
<li><a href="#election-restriction">Election restriction</a></li>
<li><a href="#committing-entries-from-previous-terms">Committing entries
from previous terms</a></li>
<li><a href="#safety-argument">Safety argument</a></li>
</ul></li>
<li><a href="#failure-scenarios">Failure Scenarios</a>
<ul>
<li><a href="#follower-and-candidate-crash">Follower and candidate
Crash</a></li>
<li><a href="#timing-and-availability">Timing and availability</a></li>
</ul></li>
<li><a href="#cluster-membership-change">Cluster membership
change</a></li>
<li><a href="#log-compaction">Log compaction</a></li>
<li><a href="#client-interaction">Client Interaction</a></li>
<li><a href="#conclusion-author-of-paper">Conclusion (Author of
paper)</a></li>
<li><a href="#what-i-learned-from-this-paper-">What I learned from this
paper ?</a></li>
</ul></li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>Rise of Algorithm
<ul>
<li>Consensus algorithm arise in the context of Replicated State
Machines; where we required to maintain same state on multiple
servers</li>
</ul></li>
<li>Consensus algorithm helps
<ul>
<li>A collection of machines to work as a coherent(सुसंगठित) or a single
logical group</li>
</ul></li>
<li>Algorithm brings
<ul>
<li>Consistency: Ensuring that all machines in the system agree on the
same state despite failures</li>
<li>Availability: Allowing the system to remain operational despite
failures of individual servers.</li>
<li>Partition Tolerance: Handling network partitions gracefully without
compromising the system’s operation.</li>
</ul></li>
<li>What are the benefits from such an algorithm ?
<ul>
<li>Ensures the reliability of large-scale software systems.</li>
<li>Makes hardware faults invisible to the client.</li>
</ul></li>
<li>What is RAFT ?
<ul>
<li>I’m a consensus algorithm designed to ensure consistent replication
of logs across servers</li>
<li>Much easier to understand compare to classic consensus algo
<strong>Paxos</strong> (by Leslie Lamport)
<ul>
<li>Paxos is complex to understand + less intuitive; RAFT is easier to
understand (Breakdown to smaller problems)</li>
</ul></li>
</ul></li>
</ul>
<h2 id="replicated-state-machines">Replicated State Machines</h2>
<figure>
<img src="./images/raft/image.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure>
<ul>
<li>What is Replicated State Machine ?
<ul>
<li>Some initial state <em>S</em></li>
<li>Log (or command)</li>
<li>Apply log on state
<ul>
<li><code>s[i]</code>:<code>log[i]</code> -&gt;
<code>s[i + 1]</code></li>
</ul></li>
</ul></li>
<li>What does logs contains ?
<ul>
<li>Sequence of commands</li>
<li>Each compute/server contains commands in the same sequence
order</li>
</ul></li>
<li>Properties of State Machine
<ul>
<li>Deterministic
<ul>
<li>Initial State S; apply log L; Final State D</li>
</ul></li>
</ul></li>
<li>Hallucination to Client!!
<ul>
<li>Once commands are properly replicated, each server’s state machine
processes them in log order, and the outputs are returned to
clients</li>
<li>RAFT make client believe server as a single, highly reliable state
machine</li>
</ul></li>
</ul>
<h2 id="must-have-properties-of-a-consensus-algorithm">Must-have
properties of a consensus algorithm</h2>
<ul>
<li>What guarantee RAFT provides ?
<ul>
<li><strong>Safety</strong>
<ul>
<li>Never return an incorrect result (under all non-Byzantine
conditions)</li>
</ul></li>
<li><strong>Available</strong>
<ul>
<li>As long as majority is there (n / 2 + 1)</li>
</ul></li>
<li><strong>Not depend on Timing</strong>
<ul>
<li>Independent of timing to ensure the consistency. Why ?
<ul>
<li>Faulty clocks</li>
<li>Extreme message delays</li>
</ul></li>
</ul></li>
</ul></li>
<li>In the common case, a command can complete as soon as a majority of
the cluster has responded to a single round of RPC
<ul>
<li>This avoids a minority of slow server to impact the performance</li>
</ul></li>
</ul>
<h2 id="main-goal-of-raft">Main GOAL of RAFT ?</h2>
<ul>
<li>MUST
<ul>
<li>Provide complete and practical foundation for system building</li>
<li>Safe under all operating conditions</li>
</ul></li>
<li>Apart from <strong>MUST</strong> the <strong>MAIN</strong> goal of
RAFT was <em>Designing for Understandability</em>
<ul>
<li>PAXOS is complex, Raft is simpler</li>
<li>Raft simplify the state space by reducing the number of states to
consider
<ul>
<li>Makes system more coherent</li>
</ul></li>
<li>Eliminating non-determinism wherever possible
<ul>
<li>At some places it helped (randomization helped Raft leader election
algorithm)</li>
</ul></li>
</ul></li>
<li>What are the claims by RAFT ?
<ul>
<li>Strong leader
<ul>
<li>Flow of log entry only from leader to other servers (followers)
which simplifies the management of logs</li>
</ul></li>
<li>Leader Election
<ul>
<li>Randomized timers to resolve conflicts (else indefinite
waiting)</li>
</ul></li>
<li>Membership change</li>
</ul></li>
</ul>
<h2 id="the-raft-consensus-algorithm">The Raft Consensus Algorithm</h2>
<ul>
<li>To implement consensus what does Raft do ?
<ul>
<li>Electing distinguished leader (two leader for same <em>term</em>
can’t exists)</li>
<li>Information flow is always from leader to followers; client always
talks with leader</li>
<li>Leader accepts logs from client and replicate them on other
server</li>
<li>Leader tells servers when it is safe to apply log entries to their
state machines</li>
</ul></li>
<li>Benefits from leader-follower concept ?
<ul>
<li>Leaders simplify the data flow</li>
<li>What if leader goes down?
<ul>
<li>If leader gets disconnected/fails; then a new leader got
elected</li>
</ul></li>
</ul></li>
</ul>
<h3 id="decomposition-of-consensus-problem">Decomposition of Consensus
Problem</h3>
<blockquote>
<p>Think your self ? - You always need a leader, and that leader should
always replicate the log. Also, the leader will become angry if some
other leader from the future changes its committed entries.</p>
</blockquote>
<ul>
<li>Leader Election
<ul>
<li>Leader keeps the Raft alive; if leader goes down, we need to pick a
new leader</li>
</ul></li>
<li>Log replication
<ul>
<li>Replication save you from sudden failures in your system e.g. hard
disk crash, network goes down, somebody plug out the cables</li>
<li>Leader must accept log entries from client and replicate them across
the cluster</li>
<li>Force other logs to agree with its own</li>
</ul></li>
<li>Safety (Once it’s served, it’s served. No taking back)
<ul>
<li>State Machine Safety
<ul>
<li>If any server has applied a particular log entry to it state
machine, then no other server may apply a different command for the same
log index</li>
<li>Ensured through election restrictions</li>
</ul></li>
</ul></li>
</ul>
<h2 id="raft-guarantees---true-at-all-times">Raft Guarantees - True at
all times!</h2>
<ul>
<li><strong>Election Safety</strong>
<ul>
<li>At most one leader in a given term <em>T</em></li>
</ul></li>
<li><strong>Leader Append-Only</strong>
<ul>
<li>A leader never overwrites or deletes entries in its own log; it only
append new entire</li>
</ul></li>
<li><strong>Log Matching</strong>
<ul>
<li>If two logs contains an entry with the same index and term, then the
logs are identical in all entries up through the given index</li>
</ul></li>
<li><strong>Leader Completeness</strong>
<ul>
<li>If a log entry is committed in a given term, then that entry will be
present in the logs of the leaders for all higher-numbered term</li>
</ul></li>
<li><strong>State Machine Safety</strong>
<ul>
<li>If a server has applied a log entry at a given index to its state
machine, no other server will ever apply a different log entry for the
same index</li>
</ul></li>
</ul>
<h2 id="raft-basics">Raft Basics</h2>
<ul>
<li>Raft cluster consists of servers (typically 5 servers)
<ul>
<li>Survive two server failures</li>
</ul></li>
<li>At a given time server is in three states
<ul>
<li>Leader</li>
<li>Follower</li>
<li>Candidate</li>
<li><figure>
<img src="./images/raft/image-1.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure></li>
</ul></li>
<li>Normal scenario
<ul>
<li>one leader, rest all are followers</li>
</ul></li>
<li>Client always talks with leader
<ul>
<li>If client talks with a follower, the follower redirect client to the
leader</li>
</ul></li>
<li>Raft divides times into <em>terms</em> of arbitrary length
<ul>
<li>Term acts as <em>logical clocks</em></li>
<li>Helps servers to detect obsolete information such as stale
leaders</li>
<li><figure>
<img src="./images/raft/image-2.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure></li>
</ul></li>
<li>Terms are numbered with consecutive integers</li>
<li>Each term begin with an election, in which one or more candidate
attempts to become leader
<ul>
<li>If candidate wins, then leader for the entire term</li>
<li>If Split votes then no leader for the term (term++; election
again)</li>
</ul></li>
<li>Raft ensures that there is at <em>most one</em> leader each
<strong>term</strong></li>
<li>Transition b/w terms may be observed at different times</li>
<li>Raft server communicate using RPCs</li>
<li>Three type of RPCs
<ul>
<li>RequestVote RPCs (by candidates)</li>
<li>AppendEntries RPCs (by leader to replicate log entries and to
provide a form of heartbeat)</li>
<li>RPC for transferring snapshots between servers</li>
</ul></li>
<li>Server retry the RPC if do not receive a response in a timely manner
and for performance: parallel RPCs</li>
<li><figure>
<img src="./images/raft/image-3.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure></li>
</ul>
<h2 id="leader-election">Leader Election</h2>
<ul>
<li><p>All server startup as followers</p>
<ul>
<li>Remains in followers as long as it receives valid RPCs from a leader
or candidate</li>
</ul></li>
<li><p>Leader send periodic heartbeat (AppendEntries RPCs with no
logs)</p></li>
<li><p>If follower receives no communication till the <em>election
timeout</em>, then it assumes there is no viable leader and begins an
election to choose a new leader</p></li>
<li><p>To begin an election</p>
<ul>
<li>Follower increment its current term to
<code>currentTerm + 1</code></li>
<li>Vote to itself (self obsessed)</li>
<li>Issue RequestVote RPCs in <strong>parallel</strong> to each of the
other servers in the cluster</li>
<li>Loop in same state until
<ul>
<li>Wins
<ul>
<li>Got majority of votes</li>
</ul></li>
<li>Loose
<ul>
<li>Another server establishes itself as leader</li>
</ul></li>
<li>Draw
<ul>
<li>No winner</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>How RAFT decrease the probability of split votes(विभाजित मत)
?</p>
<ul>
<li>Raft uses <em>randomized election timeout</em> to ensure that split
votes are rare
<ul>
<li>In later section we will see a bounded range for this <em>randomized
election timeout</em>. If not selected carefully it can affect the
liveness/performance of the Raft</li>
</ul></li>
<li>Two strategy
<ul>
<li>First
<ul>
<li>Set timeout randomly from a fixed range i.e [150-300ms]</li>
<li>This will ensure that not every one will timeout together</li>
</ul></li>
<li>Second (candidate side)
<ul>
<li>Each candidate restarts it randomized election timeout at the start
of an election, and it waits for that timeout to elapse before starting
a new election ## State of a Server</li>
</ul></li>
</ul></li>
</ul></li>
<li><p><strong>Persisted state on all servers</strong> (before
responding to RPCs)</p>
<ul>
<li><code>currentTerm</code>
<ul>
<li>Latest term server has seen</li>
<li>Initialize to 0 on first boot, increase monotonically</li>
<li>Used to ensure that terms only increase, &amp; to detect RPCs from
stale leaders &amp; candidates(logical clock)</li>
</ul></li>
<li><code>votedFor</code>
<ul>
<li>Candidate that received vote in current term (or null if none)</li>
<li>If it didn’t store to whom it voted for current term, then there is
a possibility of submitting vote twice in same term. This could lead to
two leaders in the same term(breaks election safety).</li>
</ul></li>
<li><code>log[]</code>
<ul>
<li>Log entries</li>
<li>Each entry contains (command for state machine, term when entry was
received)</li>
<li>One-based indexing</li>
<li>If the server is in leader’s majority for committing an entry, must
remember entry despite reboot, so that any future leader is guaranteed
to see committed log entry (ensure leader completeness property).
Moreover, logs play a crucial role in electing leader for a given
term.</li>
</ul></li>
</ul></li>
<li><p><strong>Volatile state on all server</strong></p>
<ul>
<li><code>commitIndex</code>
<ul>
<li>Index of highest entry known to be committed (init = 0; increase
monotonically)</li>
<li>why not persist ? Supplied from the leader</li>
</ul></li>
<li><code>lastApplied</code>
<ul>
<li>Index of highest log entry applied to the state machine (init = 0;
increase monotonically)</li>
<li>why not persist ? Computed locally, equals to the index till which
logs are applied to state machine (&lt;=commitIndex)</li>
</ul></li>
</ul></li>
<li><p><strong>Volatile state on leaders</strong></p>
<ul>
<li><code>nextIndex[]</code>
<ul>
<li>For each server
<ul>
<li>Index of the next log entry to send to that server</li>
<li>Init = leader last log index + 1</li>
</ul></li>
</ul></li>
<li><code>matchIndex[]</code>
<ul>
<li>For each server
<ul>
<li>Index of highest log entry known to be replicated on server</li>
<li>init = 0; increase monotonically</li>
</ul></li>
</ul></li>
<li>why not persist ? computed based on responses received for
AppendRPCs sent to the followers</li>
<li>why different <code>nextIndex</code> &amp; <code>matchIndex</code> ?
<ul>
<li>they have a different usecase nextIndex is used for sending log
entries to the follower</li>
<li>whereas matchIndex is used by the leader for committing log entries
## AppendEntries RPC Invoked by leader to replicate log entries + heart
beats</li>
</ul></li>
</ul></li>
</ul>
<table>
<thead>
<tr>
<th><strong>Arguments</strong></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>term</td>
<td>leader’s term</td>
</tr>
<tr>
<td>leaderId</td>
<td>so follower can redirect clients</td>
</tr>
<tr>
<td>prevLogIndex</td>
<td>index of log entry immediately preceding new ones</td>
</tr>
<tr>
<td>prevLogTerm</td>
<td>term of prevLogIndex entry</td>
</tr>
<tr>
<td>entries[]</td>
<td>log entries to store (empty in case of heartbeat)</td>
</tr>
<tr>
<td>leaderCommit</td>
<td>leader’s commitIndex</td>
</tr>
</tbody>
</table>
<table>
<colgroup>
<col style="width: 57%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr>
<th><strong>Results</strong></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>term</td>
<td><code>currentTerm</code>, for leader to update itself</td>
</tr>
<tr>
<td>success</td>
<td><code>true</code> if follower contained entry matching prevLogIndex
and prevLogTerm</td>
</tr>
</tbody>
</table>
<p><strong>Receiver Implementation</strong> 1. Reply <code>false</code>
if <code>term &lt; currentTerm</code>. - This means that if the leader
is operating on an older term than the server’s current term, return
false. 2. Reply <code>false</code> if the log does not contain an entry
at prevLogIndex with a term that matches prevLogTerm. - This ensures
that the server logs can synchronize with the leader’s logs. 3. If an
existing entry conflicts with a new one (same index but different term),
delete the existing entry and all entries that follow it. - There is a
single source of truth, and the current leader defines that truth. -
Append any new entries that are not already in the log. - If
<code>leaderCommit &gt; commitIndex</code>, set <code>commitIndex</code>
to <code>min(leaderCommit, index of the last new entry)</code>. - Why
safe ?</p>
<h2 id="requestvote-rpc">RequestVote RPC</h2>
<p>Invoked by candidates to gather vote</p>
<table>
<thead>
<tr>
<th><strong>Argument</strong></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>term</td>
<td>candidate’s term</td>
</tr>
<tr>
<td>candidateId</td>
<td>candidate requesting vote</td>
</tr>
<tr>
<td>lastLogIndex</td>
<td>index of candidate’s last log entry</td>
</tr>
<tr>
<td>lastLogTerm</td>
<td>term of candidate’s last log entry</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th><strong>Results</strong></th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>term</td>
<td><code>currentTerm</code>, for the candidate to update itself</td>
</tr>
<tr>
<td>voteGranted</td>
<td><code>true</code> means candidate received vote</td>
</tr>
</tbody>
</table>
<p><strong>Receiver Implementation</strong> 1. Reply <code>false</code>
if <code>term &lt; currentTerm</code> 2. If
<code>votedFor == null</code> or <code>candidateId</code>, and
candidate’s log is at least as up-to-date as receiver’s log, grant
vote</p>
<h2 id="rules-for-servers">Rules for Servers</h2>
<p><strong>All servers</strong> - If
<code>commitIndex &gt; lastApplied</code>, then increment
<code>lastApplied</code>, apply <code>log[lastApplied]</code> to state
machine - If RPC request or response contains term
<code>T &gt; currentTerm</code> set <code>currentTerm = T</code>,
convert to follower</p>
<p><strong>Followers</strong> - Respond to RPCs from candidates and
leaders - If election timeout elapses without receiving AppendEntries
RPC from current leader or granting vote to candidate: convert to
candidate</p>
<p><strong>Candidates</strong> - What happen in Candidate state ? - On
conversion to candidate, start election: - Increment
<code>currentTerm</code> - Vote for Self (self-obsessed) - Reset
election timer - Send RequestVote RPCs to all other servers - If vote
from majority (promote to leader) - If AppendEntries RPC received from
new leader: convert to follower - If election timeout elapses: start new
election</p>
<p><strong>Leaders</strong> - How leader prevents election timeouts ? -
Upon election: send initial empty AppendEntries RPCs (heartbeat) to each
server - When does leader response back to the client ? - If command
received from client: append entry to local log, respond after entry
applied to state machine - What does leader do to bring every followers
<code>logs[]</code> in sync? - If Leader last log index is greater than
<code>nextIndex</code> of some follower f
i.e. <code>len(leader_log) &gt; nextIndex[f]</code>; send AppendEntries
RPC with log entries starting at <code>nextIndex[f]</code> - If
successful: Update <code>nextIndex[f]</code> and
<code>matchIndex[f]</code> for the follower f - If fails because of log
inconsistency then,
<code>java             // Follower:             appendEntryRequest(rpcPayload={ni=nextIndex[f], newLog,..}) {                 if (logOfFollower[ni - 1] != logOfLeader[ni - 1]) {                     reply("Prefix of log not matching")                 } else {                     append(logOfFollower, newLog);                 }             }</code>
- In the above case, leader will decrement nextIndex and retry again -
When does leader update its <code>commitIndex</code> ? - If there exists
an <code>N</code> such that <code>N &gt; commitIndex</code> (consider N
as length of log at the leader) - And a majority of
<code>matchIndex[f] &gt;= N</code> - And
<code>log[N].term == currentTerm</code> - then set
<code>commitIndex = N</code></p>
<h2 id="logs-replication">Logs Replication</h2>
<ul>
<li>Client request contains a command to be executed by the replicated
state machine</li>
<li>The leader append the command to its log as a new entry, then issue
<code>AppendEntries</code> RPCs in parallel to each of the other server
to replicate</li>
<li>When safely replicated, the leader applies the entry to its state
machine and returns the result of that execution to the client</li>
<li>If followers crash or run slowly, or if network packets are lost,
the leader retries AppendEntries RPCs indefinitely (even after it has
responded to the client) until all followers eventually store all log
entries</li>
<li>Each log entry stores (term number, command)
<ul>
<li>Why term number ?
<ul>
<li>Helps to detect inconsistency in the logs</li>
</ul></li>
</ul></li>
<li>When does leader feels safe to apply a log entry to its state
machine ? &gt; Applied log entry are referred as committed
<ul>
<li>A log entry is committed once the leader that created the entry has
replicated it on majority of servers</li>
<li>All preceding entries in the leader’s log, including entries created
the previous leader</li>
</ul></li>
<li>When does followers feels safe to apply a log entry ?
<ul>
<li>Leader keeps track of the highest index it knows to be committed,
and it include that index in future AppendEntries RPCs(including
heartbeats) so that the other server eventually find out</li>
<li>Once a follower learn that a log entry is committed, it applies the
entry to its local state machine</li>
</ul></li>
<li>Properties maintains by the RAFT
<ul>
<li>If two entries in different logs have the same index and term, then
they store the same command</li>
<li>If two entries in different logs have the same index and term, then
the logs are identical in all preceding entries</li>
<li><figure>
<img src="./images/raft/images-4.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure></li>
</ul></li>
<li>Leader handles inconsistencies by forcing the followers’ logs to
duplicate its own
<ul>
<li>Conflicting entries will be overwritten with entries from the
leader’s log</li>
<li>leader maintains <code>nextIndex</code> for each follower</li>
</ul></li>
<li>To what value new leader initialize <code>nextIndex</code>?
<ul>
<li>for each follower <code>f</code>:
<code>nextIndex[f] = len(leader_log) + 1</code></li>
</ul></li>
</ul>
<h2 id="safety">Safety</h2>
<ul>
<li>How to sure that each state machine executes exactly the same
commands in the same order ?
<ul>
<li>Constraints at the Leader election : because we don’t want a stale
follower to become leader and overwrite entries</li>
</ul></li>
</ul>
<h3 id="election-restriction">Election restriction</h3>
<ul>
<li>In a leader-based consensus algorithm, the leader must eventually
store all of the committed log entries</li>
<li>How does Raft guarantee above ?
<ul>
<li><p>Raft use simple approach without the need to transfer those
entries to the leader</p></li>
<li><p>Log entry only flow in one direction, from leaders to
followers</p></li>
<li><p>Example:</p>
<ul>
<li><figure>
<img src="./images/raft/image-5.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure></li>
</ul></li>
<li><p>Raft uses the voting process to prevent a candidate from winning
an election unless its log contains all committed entries</p></li>
<li><p>Voter denies its vote if its own log is more up-to-date than that
of the candidate</p></li>
</ul></li>
<li>How does Raft determines which of two logs is more up-to-date ?
<ul>
<li>Comparing the <code>index</code> (len(log)) and <code>term</code> of
the last entries in the logs</li>
<li>If the logs have last entries with different terms, then the log
with the later term is more up-to-date</li>
<li>If the logs end with the same term, then whichever log is longer is
more up-to-date</li>
</ul></li>
</ul>
<h3 id="committing-entries-from-previous-terms">Committing entries from
previous terms</h3>
<ul>
<li>Is it ok for the leader to commit all the uncommitted entries from
its log ?
<ul>
<li>A big no!!
<ul>
<li>Chances are that committed entries may get overwritten by a future
leader</li>
<li>Leader doesn’t change the term value present in the logs from
previous leaders term</li>
</ul></li>
<li>Only log entries from the leader’s current term are committed by
counting replicas;</li>
<li>Once an entry from the current term has committed in this way, then
all prior entries are committed indirectly because of the Log Matching
Property &gt; Even when you have replicated on all the server (you are
fully sure to commit), but still Raft takes conservative approach for
simplicity</li>
</ul></li>
</ul>
<h3 id="safety-argument">Safety argument</h3>
<ul>
<li>Argue on Leader Completeness Property
<ul>
<li>Assume it doesn’t hold, then we prove a contradiction</li>
<li>Suppose the leader for term T commits a log entry from its term, but
that log entry is not store by the leader of some future term. Consider
the smallest term U &gt; T whose leader doesn’t store the entry</li>
</ul></li>
</ul>
<ol type="1">
<li>Committed entry must have been absent from leader U log at the time
of its election</li>
<li>Leader T replicated the entry on a majority of the cluster, and the
leader U received votes from a majority of the cluster. Thus, at least
one server (“the voter”) both accepted the entry from leader T and voted
for Leader U
<ul>
<li>Voter is key to reaching a contradiction</li>
<li><figure>
<img src="./images/raft/image-6.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure></li>
</ul></li>
<li>The voter must have accepted the committed entry from leader_t
before voting leader_u; other wise this server have rejected the
AppendEntries RPC request from the leader_t</li>
<li>The voter granted its vote to leader_u, so leader_u’s log must have
been as up-to-date as the voter. This leads to one of two
contradiction</li>
<li>First, the voter and U must shared the same last log term and U logs
length is at least as long as voters
<ul>
<li>Contradiction to our assumption i.e U doesn’t contains committed
entry</li>
</ul></li>
<li>Otherwise, leader’s U last log term must have been larger than voter
<ul>
<li>precise : It should be larger than T</li>
<li>Remember Leader completeness property : it means logs of U must
contains T committed entry, which is a contradiction</li>
</ul></li>
</ol>
<h2 id="failure-scenarios">Failure Scenarios</h2>
<h3 id="follower-and-candidate-crash">Follower and candidate Crash</h3>
<ul>
<li>Much easier to handle compare to leader crashes</li>
<li>Failure for both are handled in the same way</li>
<li>All RequestVote and AppendEntries RPC will be sent to it will
fail</li>
<li>Raft handles these failures by retrying indefinitely</li>
<li>Raft RPCs are idempotent, so this case no harm
<ul>
<li>Follower just ignore</li>
<li>Scenarios: If server crashes after completing an RPC but before
responding</li>
</ul></li>
</ul>
<h3 id="timing-and-availability">Timing and availability</h3>
<ul>
<li>Initial requirement: Safety must not depend on the timing;</li>
<li>The system must not produce incorrect results, just because some
event happens more quickly or slowly than expected</li>
<li>However, availability (the ability of a system to respond to client
in timely manner) must inevitably depend on timing
<ul>
<li>Raft rely on a steady leader to progress</li>
<li>If message exchanges take longer than the typical time between
server crashes, candidate will not stay up long enough to win an
election</li>
<li>Leader election is the aspect in Raft where timing is important</li>
<li>Raft will be able to elect and maintain steady leader as long as
system satisfies the below timing requirements
<ul>
<li><code>broadcastTime &lt;&lt; electionTimeout &lt;&lt; MTBF</code></li>
<li><code>broadcastTime</code>: Avg time it takes server to send RPCs in
parallel to every server in the cluster and receive there response</li>
<li><code>MTBF</code>: Mean Time Between Failures</li>
<li>If <code>electionTimeout &lt; broadcastTime</code>; then no body
will win the election;</li>
<li>If <code>electionTimeout &gt; MTBF</code>; then progress will halt,
as there is no election happen even after leader crashed</li>
</ul></li>
</ul></li>
<li>Typical <code>electionTimeout</code> Raft set is
<code>[10, 500]ms</code></li>
</ul>
<h2 id="cluster-membership-change">Cluster membership change</h2>
<ul>
<li>Above discussion was on assumption that cluster configuration is
fixed</li>
<li>In practice, it is not the case and cluster configuration change
over time
<ul>
<li>Replace server when they fail or change the degree of
replication</li>
</ul></li>
<li>We want to do above without the requirement of restart</li>
<li>For the configuration change mechanism to be safe,
<ul>
<li>No point in transition where it is possible for two leaders to be
elected</li>
<li>Unfortunately, any approach where server switch directly from the
old configuration to the new configuration is unsafe</li>
</ul></li>
<li>It isn’t possible to <strong>atomically</strong> switch all server
at once (which means chances are that cluster can potentially split into
two independent majorities during the transition)</li>
<li>In order to ensure safety, configuration changes must use a
two-phase approach
<ul>
<li>In Raft the cluster first switches to a transitional configuration
we call <em>joint consensus</em>; once the joint consensus has been
committed, the system then transition to the new configuration</li>
</ul></li>
<li>The joint consensus combines both new and old configurations:
<ul>
<li>Log entries are replicated to all servers in both configuration</li>
<li>Any server from either configuration may serve as leader</li>
<li>Agreement (for election and entry commitment) requires separate
majorities from both the new and old configuration</li>
</ul></li>
<li>Join consensus allows servers
<ul>
<li>Transition between configuration at different time without
compromising safety</li>
<li>Keeps system alive i.e client request are served even when
configuration changes are in transitions</li>
</ul></li>
<li>Cluster configuration are stored and communicated using special
entries in the replicated log
<ul>
<li>When a leader receives the configuration change request to change
from <span
class="math inline"><em>C</em><sub><em>o</em><em>l</em><em>d</em></sub></span>
to <span
class="math inline"><em>C</em><sub><em>n</em><em>e</em><em>w</em></sub></span>
it stores the configuration for the joint consensus (<span
class="math inline"><em>C</em><sub><em>o</em><em>l</em><em>d</em>, <em>n</em><em>e</em><em>w</em></sub></span>)
as a log entry and replicate that</li>
<li>Once a server adds a new configuration to its log, then it uses the
same for all future decisions (regardless of that is committed or not,
<strong>but why it is safe to use that?</strong>)</li>
<li></li>
<li></li>
</ul></li>
<li><figure>
<img src="./images/raft/image-7.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure></li>
</ul>
<h2 id="log-compaction">Log compaction</h2>
<ul>
<li>Remove obsolete information with time to avoid space wastage</li>
<li>Snap shotting is the simplest technique for compaction
<ul>
<li>Entire system state is written on a stable storage, then entire log
up that point is discarded (as well as older snapshots)</li>
<li><figure>
<img src="./images/raft/image-8.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure></li>
</ul></li>
<li>In raft each server takes snapshot independently, covering just the
committed entries in its logs</li>
<li>Raft also include small metadata into its snapshot</li>
<li>Although server must takes snapshot independently, the leader must
occasionally send snapshots to followers that lag behind</li>
<li><figure>
<img src="./images/raft/image-9.png" alt="alt text" />
<figcaption aria-hidden="true">alt text</figcaption>
</figure></li>
<li>Snapshotting approach departs from Raft’s strong leader principle,
since followers can take snapshots without the knowledge of the
leader</li>
<li>Consensus has already been reached when snapshotting, so no decision
conflicts.</li>
</ul>
<h2 id="client-interaction">Client Interaction</h2>
<h2 id="conclusion-author-of-paper">Conclusion (Author of paper)</h2>
<h2 id="what-i-learned-from-this-paper">What I learned from this paper
?</h2>
<ul>
<li>Centralized (leader/master) vs decentralized tradeoffs
<ul>
<li>If centralized, then how to handle the crash of leader/master</li>
<li>If decentralized, then how to make sure every one has consistent
information or every one is in sync about the overall state of the
system ?</li>
</ul></li>
<li>Network partitions - No communication possible ? sacrifice either
availability or consistency<br />
</li>
<li>Stragglers - One slow machine should not effect system</li>
<li>Keep system least dependent on timings of event (events are
non-deterministic)</li>
<li>State (persist) vs Stateless (only in-memory) [ not truly stateless
]
<ul>
<li>Only keep state wherever necessity; stateless systems are more
tolerant to fault because there recovery is simple.</li>
</ul></li>
<li>How to maintain consistency in distributed system, where we have
replication</li>
<li>How to ensure availability of system ?</li>
<li>Share state if a server is lagging / new server added
<ul>
<li>Tradeoff b/w Independent snapshots at each server Vs Leader
instructed logs</li>
</ul></li>
</ul>
